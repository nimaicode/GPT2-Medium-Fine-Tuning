{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5542,"status":"ok","timestamp":1734383234657,"user":{"displayName":"Hardik Desai","userId":"14226761856387981111"},"user_tz":300},"id":"Sp9PfC-zo1tl","outputId":"32159ff3-9cbc-4f9d-bcda-057e461e6d86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","# Check if GPU can be used to run code\n","device = torch.device(\n","    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqpnrU1om0bD"},"outputs":[],"source":["import os\n","\n","## Keep your training documents in a folder named 'data'\n","data_dir = \"sample_data\"\n","output_file = \"all_data.txt\"\n","\n","def is_hidden(filepath):\n","    return os.path.basename(filepath).startswith('.')\n","\n","with open(output_file, \"w\") as outfile:\n","    for filename in os.listdir(data_dir):\n","        filepath = os.path.join(data_dir, filename)\n","        if not is_hidden(filepath):\n","            with open(filepath) as infile:\n","                for line in infile:\n","                    # only write the line if it's not empty\n","                    # (and, not just whitespace)\n","                    if line.strip():\n","                        outfile.write(line)"]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"MxNHog6adXhc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7ENcy9q_dJO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, \\\n","    TrainingArguments, Trainer, DataCollatorWithPadding\n","from torch.utils.data import Dataset\n","!pip install wandb\n","## GPT-2 Small ('gpt2'): 124 million parameters.\n","## GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n","## GPT-2 Large ('gpt2-large'): 774 million parameters.\n","## GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n","\n","\n","# Load pre-trained GPT-2 tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n","\n","# Set padding token\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Your custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, tokenizer, file_path, block_size):\n","        self.tokenizer = tokenizer\n","        with open(file_path, \"r\") as f:\n","            self.text = f.read().splitlines()\n","    def __len__(self):\n","        return len(self.text)\n","    def __getitem__(self, idx):\n","        tokenized_inputs = self.tokenizer(\n","            self.text[idx],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=128,\n","            return_tensors=\"pt\")\n","        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n","        return tokenized_inputs\n","\n","# Load data\n","data = CustomDataset(tokenizer, \"all_data.txt\", 128)\n","\n","# Create a data collator that will dynamically pad the sequences\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# Training arguments and Trainer\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=2,\n","    num_train_epochs=400, # Increse for more training from the fine-tuning data\n","    learning_rate=1e-4,  # Decrease the learning rate for smaller fine-tuning data\n","    output_dir='./results',\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    load_best_model_at_end=False,\n","    evaluation_strategy=\"no\",\n","    remove_unused_columns=False,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=data,\n","    eval_dataset=None,  # You can specify an evaluation dataset here\n","    data_collator=data_collator,  # Add the data collator here\n",")\n","\n","trainer.train()\n"],"metadata":{"id":"YQVlaDqbm2VO"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}